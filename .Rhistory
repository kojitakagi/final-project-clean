library(tidyverse)
library(janitor)
install.packages("janitor")
library(janitor)
library(ggplot2)
library(knitr)
# Read the data (adjust the path if needed)
df <- read_csv("data/diabetes_binary_health_indicators_BRFSS2015.csv") %>%
clean_names()
# Read the data (adjust the path if needed)
df <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv") %>%
clean_names()
View(df)
View(df)
df <- df %>%
mutate(
diabetes_binary = factor(diabetes_binary, labels = c("No", "Yes")),
high_bp = factor(high_bp, labels = c("No", "Yes")),
smoker = factor(smoker, labels = c("No", "Yes")),
phys_activity = factor(phys_activity, labels = c("No", "Yes"))
)
View(df)
df %>%
count(high_bp, diabetes_binary) %>%
ggplot(aes(x = high_bp, y = n, fill = diabetes_binary)) +
geom_col(position = "fill") +
ylab("Proportion") +
labs(title = "Diabetes Proportion by High Blood Pressure Status")
df %>%
count(smoker, diabetes_binary) %>%
ggplot(aes(x = smoker, y = n, fill = diabetes_binary)) +
geom_col(position = "fill") +
ylab("Proportion") +
labs(title = "Diabetes Proportion by Smoking Status")
df %>%
count(phys_activity, diabetes_binary) %>%
ggplot(aes(x = phys_activity, y = n, fill = diabetes_binary)) +
geom_col(position = "fill") +
ylab("Proportion") +
labs(title = "Diabetes Proportion by Physical Activity")
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
library(rpart.plot)
install.packages("rpart.plot", type = "binary")
library(rpart.plot)
chooseCRANmirror()
install.packages("rpart.plot", type = "binary")
library(rpart.plot)
library(randomForest)
library(janitor)
library(MLmetrics)  # for LogLoss if needed
install.packages("MLmetrics")
library(MLmetrics)  # for LogLoss if needed
set.seed(2025)
df <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv") %>%
clean_names()
# Convert variables to factors where appropriate
df <- df %>%
mutate(
diabetes_binary = factor(diabetes_binary, labels = c("No", "Yes")),
high_bp = factor(high_bp),
smoker = factor(smoker),
phys_activity = factor(phys_activity)
)
# Convert variables to factors where appropriate
df <- df %>%
mutate(
diabetes_binary = factor(diabetes_binary, labels = c("No", "Yes")),
high_bp = factor(high_bp),
smoker = factor(smoker),
phys_activity = factor(phys_activity)
)
set.seed(2025)
train_idx <- createDataPartition(df$diabetes_binary, p = 0.7, list = FALSE)
train_data <- df[train_idx, ]
test_data  <- df[-train_idx, ]
# Three different models
logit1 <- glm(diabetes_binary ~ high_bp, data = train_data, family = "binomial")
logit2 <- glm(diabetes_binary ~ high_bp + smoker, data = train_data, family = "binomial")
logit3 <- glm(diabetes_binary ~ high_bp + smoker + phys_activity, data = train_data, family = "binomial")
# Cross-validation
ctrl <- trainControl(method = "cv", number = 5, summaryFunction = mnLogLoss, classProbs = TRUE)
logit_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "glm",
family = "binomial",
trControl = ctrl,
metric = "logLoss"
)
logit_model
tree_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "rpart",
trControl = ctrl,
tuneLength = 10,
metric = "logLoss"
)
rpart.plot(tree_model$finalModel)
tree_model
rf_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "rf",
trControl = ctrl,
tuneLength = 5,
metric = "logLoss"
)
tree_grid <- expand.grid(cp = seq(0.001, 0.05, by = 0.005))
tree_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "rpart",
trControl = ctrl,
tuneGrid = tree_grid,
metric = "logLoss"
)
rpart.plot(tree_model$finalModel)
tree_model
rpart.plot(tree_model$finalModel)
tree_model
rf_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "rf",
trControl = ctrl,
tuneLength = 5,
metric = "logLoss"
)
rf_model
rf_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "rf",
trControl = ctrl,
tuneLength = 5,
metric = "logLoss"
)
rf_model
rf_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "rf",
trControl = ctrl,
tuneLength = 2,
metric = "logLoss",
ntree = 100
)
rf_model
predict_diabetes <- function(high_bp = 0, smoker = 0, phys_activity = 1) {
new_data <- data.frame(
high_bp = high_bp,
smoker = smoker,
phys_activity = phys_activity
)
predict(logit_model, newdata = new_data, type = "prob")
}
# Load required libraries
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(janitor)
library(MLmetrics)
# Set seed for reproducibility
set.seed(2025)
# Load and clean data
df <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv") %>%
clean_names()
# Convert relevant variables to factors
df <- df %>%
mutate(
diabetes_binary = factor(diabetes_binary, labels = c("No", "Yes")),
high_bp = factor(high_bp),
smoker = factor(smoker),
phys_activity = factor(phys_activity)
)
# Train-test split
train_idx <- createDataPartition(df$diabetes_binary, p = 0.7, list = FALSE)
train_data <- df[train_idx, ]
test_data  <- df[-train_idx, ]
# Define training control
ctrl <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = mnLogLoss
)
# Fit logistic regression model
logit_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "glm",
family = "binomial",
trControl = ctrl,
metric = "logLoss"
)
# Fit random forest model
rf_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "rf",
trControl = ctrl,
tuneLength = 2,
metric = "logLoss",
ntree = 100
)
# Define prediction function using the better model (logit_model)
predict_diabetes <- function(high_bp = 0, smoker = 0, phys_activity = 1) {
new_data <- data.frame(
high_bp = factor(high_bp, levels = c(0, 1)),
smoker = factor(smoker, levels = c(0, 1)),
phys_activity = factor(phys_activity, levels = c(0, 1))
)
predict(logit_model, newdata = new_data, type = "prob")
}
predict_diabetes
rf_model
logit_model
# Fit classification tree model
tree_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "rpart",
trControl = ctrl,
tuneLength = 10,
metric = "logLoss"
)
# Visualize the tree
rpart.plot(tree_model$finalModel)
# Predict probabilities for each model
logit_prob <- predict(logit_model, newdata = test_data, type = "prob")[, "Yes"]
tree_prob  <- predict(tree_model,  newdata = test_data, type = "prob")[, "Yes"]
rf_prob    <- predict(rf_model,    newdata = test_data, type = "prob")[, "Yes"]
# Convert true labels
y_true <- ifelse(test_data$diabetes_binary == "Yes", 1, 0)
# Compute logLoss
tibble(
Model = c("Logistic Regression", "Classification Tree", "Random Forest"),
LogLoss = c(
LogLoss(logit_prob, y_true),
LogLoss(tree_prob, y_true),
LogLoss(rf_prob, y_true)
)
)
tree_model$finalModel
tree_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "rpart",
trControl = ctrl,
tuneGrid = data.frame(cp = seq(0.0005, 0.01, by = 0.001)),
metric = "logLoss"
)
plot(tree_model)  # cp vs logLoss の関係が見える
tree_model_raw <- rpart(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "class",
control = rpart.control(cp = 0.001)
)
rpart.plot(tree_model_raw)
tree_model_raw <- rpart(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "class",
control = rpart.control(cp = 0.001)
)
rpart.plot(tree_model_raw)
ctrl <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary
)
tree_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "rpart",
trControl = ctrl,
metric = "Accuracy",
tuneLength = 10
)
tree_model <- train(
diabetes_binary ~ high_bp + smoker + phys_activity,
data = train_data,
method = "rpart",
trControl = ctrl,
tuneGrid = data.frame(cp = seq(0.0005, 0.01, by = 0.001)),
metric = "logLoss"
)
plot(tree_model)  # cp vs logLoss の関係が見える
library(vip)
install.packages("vip")
library(vip)
logit_model <- glm(diabetes_binary ~ ., data = train_data, family = binomial)
vip::vi_model(logit_model)
diabetes_binary ~ high_bp + bmi + high_chol + chol_check + heart_diseaseor_attack + stroke
tree_model2 <- train(
diabetes_binary ~ high_bp + bmi + high_chol + chol_check + heart_diseaseor_attack + stroke,
data = train_data,
method = "rpart",
trControl = ctrl,
tuneLength = 10,
metric = "logLoss"
)
ctrl <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary  # ROCを使うための設定
)
tree_model2 <- train(
diabetes_binary ~ high_bp + bmi + high_chol + chol_check + heart_diseaseor_attack + stroke,
data = train_data,
method = "rpart",
trControl = ctrl,
tuneLength = 10,
metric = "ROC"
)
tree_model2
rpart.plot(tree_model2$finalModel)
tree2_pred_prob <- predict(tree_model2, newdata = test_data, type = "prob")[, "Yes"]
y_true <- ifelse(test_data$diabetes_binary == "Yes", 1, 0)
library(MLmetrics)
tree2_logloss <- LogLoss(tree2_pred_prob, y_true)
tree2_pred_class <- predict(tree_model2, newdata = test_data)
conf_matrix <- confusionMatrix(tree2_pred_class, test_data$diabetes_binary)
tree2_accuracy <- conf_matrix$overall["Accuracy"]
tree2_sensitivity <- conf_matrix$byClass["Sensitivity"]
tree2_specificity <- conf_matrix$byClass["Specificity"]
results <- tibble(
Model = c("Logistic Regression", "Classification Tree (basic)", "Classification Tree (6 vars)", "Random Forest"),
LogLoss = c(
LogLoss(logit_prob, y_true),
LogLoss(tree_prob, y_true),
tree2_logloss,
LogLoss(rf_prob, y_true)
)
)
print(results)
install.packages("skimr")
# Load necessary libraries
library(plumber)
install.packages("plumber")
# Load necessary libraries
library(plumber)
library(tidyverse)
library(caret)
library(janitor)
# Load and prepare data
df <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv") %>%
clean_names() %>%
mutate(
diabetes_binary = factor(diabetes_binary, labels = c("No", "Yes")),
high_bp = factor(high_bp),
high_chol = factor(high_chol),
chol_check = factor(chol_check),
bmi = as.numeric(bmi)
)
# Train-test split (only training needed here)
set.seed(2025)
train_idx <- createDataPartition(df$diabetes_binary, p = 0.7, list = FALSE)
train_data <- df[train_idx, ]
# Fit the best model (logistic regression)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = mnLogLoss)
final_model <- train(
diabetes_binary ~ high_bp + bmi + high_chol + chol_check,
data = train_data,
method = "glm",
family = "binomial",
trControl = ctrl,
metric = "logLoss"
)
#* Predict diabetes probability
#* @param high_bp 0 or 1 (default = most common: 1)
#* @param bmi numeric (default = 28.5)
#* @param high_chol 0 or 1 (default = 1)
#* @param chol_check 0 or 1 (default = 1)
#* @get /predict
function(high_bp = 1, bmi = 28.5, high_chol = 1, chol_check = 1) {
new_data <- data.frame(
high_bp = factor(as.numeric(high_bp), levels = c(0, 1)),
bmi = as.numeric(bmi),
high_chol = factor(as.numeric(high_chol), levels = c(0, 1)),
chol_check = factor(as.numeric(chol_check), levels = c(0, 1))
)
probs <- predict(final_model, newdata = new_data, type = "prob")
return(list(probability_yes = probs$Yes))
}
#* Info endpoint
#* @get /info
function() {
list(
name = "Koji Takagi",
github_pages_url = "https://yourusername.github.io/yourproject/"
)
}
#* Info endpoint
#* @get /info
function() {
list(
name = "Koji Takagi",
github_pages_url = "https://github.com/kojitakagi/final-project.git"
)
}
# plumber.R
# Load necessary libraries
library(plumber)
library(tidyverse)
library(caret)
library(janitor)
# Load and prepare data
df <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv") %>%
clean_names() %>%
mutate(
diabetes_binary = factor(diabetes_binary, labels = c("No", "Yes")),
high_bp = factor(high_bp),
high_chol = factor(high_chol),
chol_check = factor(chol_check),
bmi = as.numeric(bmi)
)
# Train-test split (only training needed here)
set.seed(2025)
train_idx <- createDataPartition(df$diabetes_binary, p = 0.7, list = FALSE)
train_data <- df[train_idx, ]
# Fit the best model (logistic regression)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = mnLogLoss)
final_model <- train(
diabetes_binary ~ high_bp + bmi + high_chol + chol_check,
data = train_data,
method = "glm",
family = "binomial",
trControl = ctrl,
metric = "logLoss"
)
#* Predict diabetes probability
#* @param high_bp 0 or 1 (default = most common: 1)
#* @param bmi numeric (default = 28.5)
#* @param high_chol 0 or 1 (default = 1)
#* @param chol_check 0 or 1 (default = 1)
#* @get /predict
function(high_bp = 1, bmi = 28.5, high_chol = 1, chol_check = 1) {
new_data <- data.frame(
high_bp = factor(as.numeric(high_bp), levels = c(0, 1)),
bmi = as.numeric(bmi),
high_chol = factor(as.numeric(high_chol), levels = c(0, 1)),
chol_check = factor(as.numeric(chol_check), levels = c(0, 1))
)
probs <- predict(final_model, newdata = new_data, type = "prob")
return(list(probability_yes = probs$Yes))
}
#* Info endpoint
#* @get /info
function() {
list(
name = "Koji Takagi",
github_pages_url = "https://github.com/kojitakagi/final-project.git"
)
}
library(plumber)
pr("plumber.R") %>% run(port = 8000)
# plumberルーターを定義
api <- pr("plumber.R")
# APIを起動（ポート8000）
api$run(port = 8000)
library(plumber)
api <- pr("plumber.R")
api$run(port = 8000)
library(plumber)
api <- pr("plumber.R")
api$run(port = 8000)
library(plumber)
api <- pr("plumber.R")
api$run(port = 8000)
api <- pr("plumber.R")
api$run(port = 8000)
api <- pr("plumber.R")
api$run(port = 8000)
docker build -t diabetes-api .
docker save -o diabetes-api.tar diabetes-api
