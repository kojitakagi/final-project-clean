{
  "hash": "c1602c94767213464155f91feac8ff81",
  "result": {
    "markdown": "---\ntitle: \"Modeling for Diabetes Prediction\"\nauthor: \"Koji Takagi\"\nformat: html\n---\n\n\n\n\n## 1. Introduction\n\nIn this document, we model the probability of diabetes using health indicators from the 2015 BRFSS dataset. We use logistic regression, classification trees, and random forests to model the binary outcome `diabetes_binary`. Our goal is to identify the best-performing model using 5-fold cross-validation and logLoss as the performance metric.\n\n## 2. Data Preparation\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\") %>%\n  clean_names() %>%\n  mutate(\n    diabetes_binary = factor(diabetes_binary, labels = c(\"No\", \"Yes\")),\n    high_bp = factor(high_bp),\n    smoker = factor(smoker),\n    phys_activity = factor(phys_activity),\n    bmi = as.numeric(bmi),\n    high_chol = factor(high_chol),\n    chol_check = factor(chol_check),\n    heart_diseaseor_attack = factor(heart_diseaseor_attack),\n    stroke = factor(stroke)\n  )\n\n# Split data\nset.seed(2025)\ntrain_idx <- createDataPartition(df$diabetes_binary, p = 0.7, list = FALSE)\ntrain_data <- df[train_idx, ]\ntest_data  <- df[-train_idx, ]\n```\n:::\n\n\n## 3. Logistic Regression\n\n### Explanation\nA logistic regression model is suitable for binary outcomes, such as predicting diabetes status. It estimates the log-odds of the outcome as a linear combination of the predictors.\n\n### Model Fitting\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl <- trainControl(method = \"cv\", number = 5, classProbs = TRUE, summaryFunction = mnLogLoss)\n\nlogit_model1 <- train(diabetes_binary ~ high_bp + smoker + phys_activity,\n                      data = train_data, method = \"glm\", family = \"binomial\",\n                      trControl = ctrl, metric = \"logLoss\")\n\nlogit_model2 <- train(diabetes_binary ~ high_bp + bmi + phys_activity,\n                      data = train_data, method = \"glm\", family = \"binomial\",\n                      trControl = ctrl, metric = \"logLoss\")\n\nlogit_model3 <- train(diabetes_binary ~ high_bp + bmi + high_chol + chol_check,\n                      data = train_data, method = \"glm\", family = \"binomial\",\n                      trControl = ctrl, metric = \"logLoss\")\n```\n:::\n\n\n### Best Logistic Model\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_models <- list(Model1 = logit_model1, Model2 = logit_model2, Model3 = logit_model3)\nsapply(logit_models, function(m) min(m$results$logLoss))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Model1    Model2    Model3 \n0.3646704 0.3547407 0.3475828 \n```\n:::\n\n```{.r .cell-code}\nlogit_model <- logit_model3  # Assuming this one has the lowest logLoss\n```\n:::\n\n\nThe model using `high_bp`, `bmi`, `high_chol`, and `chol_check` (Model 3) achieved the lowest logLoss and is selected as the best logistic regression model.\n\n## 4. Classification Tree\n\n### Explanation\nClassification trees split the data based on predictor values to create homogeneous subgroups. They are easy to interpret but prone to overfitting.\n\n### Model Fitting\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_grid <- expand.grid(cp = seq(0.001, 0.05, length.out = 10))\ntree_model <- train(\n  diabetes_binary ~ high_bp + bmi + high_chol + chol_check + heart_diseaseor_attack + stroke,\n  data = train_data,\n  method = \"rpart\",\n  trControl = ctrl,\n  tuneGrid = tree_grid,\n  metric = \"logLoss\"\n)\n```\n:::\n\n\n### Visualization\n\n::: {.cell}\n\n```{.r .cell-code}\nrpart.plot(tree_model$finalModel)\n```\n\n::: {.cell-output-display}\n![](Modeling_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## 5. Random Forest\n\n### Explanation\nA random forest builds multiple decision trees using bootstrapped samples and random subsets of predictors, improving stability and accuracy.\n\n### Model Fitting\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_grid <- expand.grid(mtry = 2:4)\nrf_model <- train(\n  diabetes_binary ~ high_bp + bmi + high_chol + chol_check + heart_diseaseor_attack + stroke,\n  data = train_data,\n  method = \"rf\",\n  trControl = ctrl,\n  tuneGrid = rf_grid,\n  metric = \"logLoss\",\n  ntree = 100\n)\n```\n:::\n\n\n## 6. Final Model Comparison\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_prob <- predict(logit_model, newdata = test_data, type = \"prob\")[, \"Yes\"]\ntree_prob  <- predict(tree_model,  newdata = test_data, type = \"prob\")[, \"Yes\"]\nrf_prob    <- predict(rf_model,    newdata = test_data, type = \"prob\")[, \"Yes\"]\n\ny_true <- ifelse(test_data$diabetes_binary == \"Yes\", 1, 0)\n\nresults <- tibble(\n  Model = c(\"Logistic Regression\", \"Classification Tree\", \"Random Forest\"),\n  LogLoss = c(\n    LogLoss(logit_prob, y_true),\n    LogLoss(tree_prob, y_true),\n    LogLoss(rf_prob, y_true)\n  )\n)\n\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 2\n  Model               LogLoss\n  <chr>                 <dbl>\n1 Logistic Regression   0.347\n2 Classification Tree   0.358\n3 Random Forest         3.86 \n```\n:::\n:::\n\n\n## 7. Final Thoughts\nBased on logLoss on the test set, we select the model with the lowest logLoss as the final model for deployment or interpretation. According to the comparison, the **Logistic Regression** model had the best performance.\n\n\n---\n[Back to EDA Page](EDA.html)\n",
    "supporting": [
      "Modeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}