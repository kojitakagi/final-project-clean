{"title":"Modeling for Diabetes Prediction","markdown":{"yaml":{"title":"Modeling for Diabetes Prediction","author":"Koji Takagi","format":"html"},"headingText":"1. Introduction","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(caret)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(randomForest)\nlibrary(MLmetrics)\nset.seed(2025)\n```\n\n\nIn this document, we model the probability of diabetes using health indicators from the 2015 BRFSS dataset. We use logistic regression, classification trees, and random forests to model the binary outcome `diabetes_binary`. Our goal is to identify the best-performing model using 5-fold cross-validation and logLoss as the performance metric.\n\n## 2. Data Preparation\n\n```{r}\ndf <- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\") %>%\n  clean_names() %>%\n  mutate(\n    diabetes_binary = factor(diabetes_binary, labels = c(\"No\", \"Yes\")),\n    high_bp = factor(high_bp),\n    smoker = factor(smoker),\n    phys_activity = factor(phys_activity),\n    bmi = as.numeric(bmi),\n    high_chol = factor(high_chol),\n    chol_check = factor(chol_check),\n    heart_diseaseor_attack = factor(heart_diseaseor_attack),\n    stroke = factor(stroke)\n  )\n\n# Split data\nset.seed(2025)\ntrain_idx <- createDataPartition(df$diabetes_binary, p = 0.7, list = FALSE)\ntrain_data <- df[train_idx, ]\ntest_data  <- df[-train_idx, ]\n```\n\n## 3. Logistic Regression\n\n### Explanation\nA logistic regression model is suitable for binary outcomes, such as predicting diabetes status. It estimates the log-odds of the outcome as a linear combination of the predictors.\n\n### Model Fitting\n```{r}\nctrl <- trainControl(method = \"cv\", number = 5, classProbs = TRUE, summaryFunction = mnLogLoss)\n\nlogit_model1 <- train(diabetes_binary ~ high_bp + smoker + phys_activity,\n                      data = train_data, method = \"glm\", family = \"binomial\",\n                      trControl = ctrl, metric = \"logLoss\")\n\nlogit_model2 <- train(diabetes_binary ~ high_bp + bmi + phys_activity,\n                      data = train_data, method = \"glm\", family = \"binomial\",\n                      trControl = ctrl, metric = \"logLoss\")\n\nlogit_model3 <- train(diabetes_binary ~ high_bp + bmi + high_chol + chol_check,\n                      data = train_data, method = \"glm\", family = \"binomial\",\n                      trControl = ctrl, metric = \"logLoss\")\n```\n\n### Best Logistic Model\n```{r}\nlogit_models <- list(Model1 = logit_model1, Model2 = logit_model2, Model3 = logit_model3)\nsapply(logit_models, function(m) min(m$results$logLoss))\nlogit_model <- logit_model3  # Assuming this one has the lowest logLoss\n```\n\nThe model using `high_bp`, `bmi`, `high_chol`, and `chol_check` (Model 3) achieved the lowest logLoss and is selected as the best logistic regression model.\n\n## 4. Classification Tree\n\n### Explanation\nClassification trees split the data based on predictor values to create homogeneous subgroups. They are easy to interpret but prone to overfitting.\n\n### Model Fitting\n```{r}\ntree_grid <- expand.grid(cp = seq(0.001, 0.05, length.out = 10))\ntree_model <- train(\n  diabetes_binary ~ high_bp + bmi + high_chol + chol_check + heart_diseaseor_attack + stroke,\n  data = train_data,\n  method = \"rpart\",\n  trControl = ctrl,\n  tuneGrid = tree_grid,\n  metric = \"logLoss\"\n)\n```\n\n### Visualization\n```{r}\nrpart.plot(tree_model$finalModel)\n```\n\n## 5. Random Forest\n\n### Explanation\nA random forest builds multiple decision trees using bootstrapped samples and random subsets of predictors, improving stability and accuracy.\n\n### Model Fitting\n```{r}\nrf_grid <- expand.grid(mtry = 2:4)\nrf_model <- train(\n  diabetes_binary ~ high_bp + bmi + high_chol + chol_check + heart_diseaseor_attack + stroke,\n  data = train_data,\n  method = \"rf\",\n  trControl = ctrl,\n  tuneGrid = rf_grid,\n  metric = \"logLoss\",\n  ntree = 100\n)\n```\n\n## 6. Final Model Comparison\n```{r}\nlogit_prob <- predict(logit_model, newdata = test_data, type = \"prob\")[, \"Yes\"]\ntree_prob  <- predict(tree_model,  newdata = test_data, type = \"prob\")[, \"Yes\"]\nrf_prob    <- predict(rf_model,    newdata = test_data, type = \"prob\")[, \"Yes\"]\n\ny_true <- ifelse(test_data$diabetes_binary == \"Yes\", 1, 0)\n\nresults <- tibble(\n  Model = c(\"Logistic Regression\", \"Classification Tree\", \"Random Forest\"),\n  LogLoss = c(\n    LogLoss(logit_prob, y_true),\n    LogLoss(tree_prob, y_true),\n    LogLoss(rf_prob, y_true)\n  )\n)\n\nresults\n```\n\n## 7. Final Thoughts\nBased on logLoss on the test set, we select the model with the lowest logLoss as the final model for deployment or interpretation. According to the comparison, the **Logistic Regression** model had the best performance.\n\n---\n[Back to EDA Page](EDA.html)\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(caret)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(randomForest)\nlibrary(MLmetrics)\nset.seed(2025)\n```\n\n## 1. Introduction\n\nIn this document, we model the probability of diabetes using health indicators from the 2015 BRFSS dataset. We use logistic regression, classification trees, and random forests to model the binary outcome `diabetes_binary`. Our goal is to identify the best-performing model using 5-fold cross-validation and logLoss as the performance metric.\n\n## 2. Data Preparation\n\n```{r}\ndf <- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\") %>%\n  clean_names() %>%\n  mutate(\n    diabetes_binary = factor(diabetes_binary, labels = c(\"No\", \"Yes\")),\n    high_bp = factor(high_bp),\n    smoker = factor(smoker),\n    phys_activity = factor(phys_activity),\n    bmi = as.numeric(bmi),\n    high_chol = factor(high_chol),\n    chol_check = factor(chol_check),\n    heart_diseaseor_attack = factor(heart_diseaseor_attack),\n    stroke = factor(stroke)\n  )\n\n# Split data\nset.seed(2025)\ntrain_idx <- createDataPartition(df$diabetes_binary, p = 0.7, list = FALSE)\ntrain_data <- df[train_idx, ]\ntest_data  <- df[-train_idx, ]\n```\n\n## 3. Logistic Regression\n\n### Explanation\nA logistic regression model is suitable for binary outcomes, such as predicting diabetes status. It estimates the log-odds of the outcome as a linear combination of the predictors.\n\n### Model Fitting\n```{r}\nctrl <- trainControl(method = \"cv\", number = 5, classProbs = TRUE, summaryFunction = mnLogLoss)\n\nlogit_model1 <- train(diabetes_binary ~ high_bp + smoker + phys_activity,\n                      data = train_data, method = \"glm\", family = \"binomial\",\n                      trControl = ctrl, metric = \"logLoss\")\n\nlogit_model2 <- train(diabetes_binary ~ high_bp + bmi + phys_activity,\n                      data = train_data, method = \"glm\", family = \"binomial\",\n                      trControl = ctrl, metric = \"logLoss\")\n\nlogit_model3 <- train(diabetes_binary ~ high_bp + bmi + high_chol + chol_check,\n                      data = train_data, method = \"glm\", family = \"binomial\",\n                      trControl = ctrl, metric = \"logLoss\")\n```\n\n### Best Logistic Model\n```{r}\nlogit_models <- list(Model1 = logit_model1, Model2 = logit_model2, Model3 = logit_model3)\nsapply(logit_models, function(m) min(m$results$logLoss))\nlogit_model <- logit_model3  # Assuming this one has the lowest logLoss\n```\n\nThe model using `high_bp`, `bmi`, `high_chol`, and `chol_check` (Model 3) achieved the lowest logLoss and is selected as the best logistic regression model.\n\n## 4. Classification Tree\n\n### Explanation\nClassification trees split the data based on predictor values to create homogeneous subgroups. They are easy to interpret but prone to overfitting.\n\n### Model Fitting\n```{r}\ntree_grid <- expand.grid(cp = seq(0.001, 0.05, length.out = 10))\ntree_model <- train(\n  diabetes_binary ~ high_bp + bmi + high_chol + chol_check + heart_diseaseor_attack + stroke,\n  data = train_data,\n  method = \"rpart\",\n  trControl = ctrl,\n  tuneGrid = tree_grid,\n  metric = \"logLoss\"\n)\n```\n\n### Visualization\n```{r}\nrpart.plot(tree_model$finalModel)\n```\n\n## 5. Random Forest\n\n### Explanation\nA random forest builds multiple decision trees using bootstrapped samples and random subsets of predictors, improving stability and accuracy.\n\n### Model Fitting\n```{r}\nrf_grid <- expand.grid(mtry = 2:4)\nrf_model <- train(\n  diabetes_binary ~ high_bp + bmi + high_chol + chol_check + heart_diseaseor_attack + stroke,\n  data = train_data,\n  method = \"rf\",\n  trControl = ctrl,\n  tuneGrid = rf_grid,\n  metric = \"logLoss\",\n  ntree = 100\n)\n```\n\n## 6. Final Model Comparison\n```{r}\nlogit_prob <- predict(logit_model, newdata = test_data, type = \"prob\")[, \"Yes\"]\ntree_prob  <- predict(tree_model,  newdata = test_data, type = \"prob\")[, \"Yes\"]\nrf_prob    <- predict(rf_model,    newdata = test_data, type = \"prob\")[, \"Yes\"]\n\ny_true <- ifelse(test_data$diabetes_binary == \"Yes\", 1, 0)\n\nresults <- tibble(\n  Model = c(\"Logistic Regression\", \"Classification Tree\", \"Random Forest\"),\n  LogLoss = c(\n    LogLoss(logit_prob, y_true),\n    LogLoss(tree_prob, y_true),\n    LogLoss(rf_prob, y_true)\n  )\n)\n\nresults\n```\n\n## 7. Final Thoughts\nBased on logLoss on the test set, we select the model with the lowest logLoss as the final model for deployment or interpretation. According to the comparison, the **Logistic Regression** model had the best performance.\n\n---\n[Back to EDA Page](EDA.html)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Modeling.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","title":"Modeling for Diabetes Prediction","author":"Koji Takagi"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}